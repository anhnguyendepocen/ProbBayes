---
title: Chapter 7.2 Bayesian Inference with Discrete Priors
author: Jim Albert and Monika Hu
date: Chapter 7 Learning About a Binomial Probability
output: 
    beamer_presentation: default
    logo: ProbBayes_cover.jpg
fontsize: 12pt
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(ggplot2)
library(ProbBayes)
library(dplyr)
crcblue <- "#2905a1"
```


## Example: students' dining preference

- A popular restaurant in a college town has been in business for about 5 years

- The restaurant owner wishes to learn more about his customers

- Interested in learning about the dining preferences of the students

- The owner plans to conduct a survey by asking students "what is your favorite day for eating out?"  

- He wants to find out what percentage of students prefer to dine on Friday

- Let $p$ denote the proportion of all students whose answer is Friday

## Discrete prior distributions for proportion $p$

- A set of plausible values of $p$:

$$p = \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}$$

- A laymen's prior distribution for $p$:

$$\pi_l(p)= (1/6, 1/6, 1/6, 1/6, 1/6, 1/6)$$

- An expert's prior distribution for $p$:
$$\pi_e(p)= (0.125, 0.125, 0.250, 0.250, 0.125, 0.125)$$

## R for $\pi_e(p)$

- The ```ProbBayes``` R package

```{r, echo = TRUE, warning = FALSE, message = FALSE, size = "footnotesize"}
bayes_table <- data.frame(p = seq(.3, .8, by=.1),
                          Prior = c(1, 1, 2, 2, 1, 1))
bayes_table
```

## R for $\pi_e(p)$ cont'd

- Use the function ```mutate()``` to normalize these weights to obtain the prior probabilities in the Prior column

```{r, echo = TRUE, warning = FALSE, message = FALSE}
bayes_table %>% 
  mutate(Prior = Prior / sum(Prior)) -> bayes_table
bayes_table
```

## R for $\pi_e(p)$ cont'd

- Plot the restaurant owner's prior distribution by use of ```ggplot2``` functions

```{r, echo = FALSE, warning = FALSE, message = FALSE}
ggplot(data=bayes_table, aes(x=p, y=Prior)) +
  geom_bar(stat="identity", fill=crcblue, width = 0.06) +
  increasefont()
```


## Likelihood

- The restaurant owner gives a survey to 20 student diners at the restaurant

- Out of the 20 student respondents, 12 say that their favorite day for eating out is Friday

- The likelihood is a function of the quantity of interest, the proportion $p$

- The owner has conducted an experiment 20 times
    - each experiment involves a "yes" or "no" answer from the respondent to the rephrased question "whether Friday is your preferred day to dine out"
    - the proportion $p$ is the probability a student answers "yes"

## Review: binomial experiment

Four conditions for a binomial experiment:

- One is repeating the same basic task or trial many times -- let the number of trials be denoted by $n$

- On each trial, there are two possible outcomes called "success" or "failure"

- The probability of a success, denoted by $p$, is the same for each trial

- The results of outcomes from different trials are independent


## The binomial likelihood function 

- The probability of $y$ successes in a Binomial experiment is given by

$$Prob(Y=y) = {n \choose y} p^y (1 - p)^{n - y}, y = 0, \cdots, n$$

- The likelihood is the chance of 12 successes in 20 trials viewed as a function of the probability of success $p$:
$$Likelihood = L(p) = {20 \choose 12} p ^ {12} (1 - p) ^ 8$$
    - generally use $L$ to denote a likelihood function
    - $L$ is a function of $p$

## R for the likelihood

- The likelihood function $L(p)$ is efficiently computed using the ```dbinom()``` function in R
    - the sample size $n$: 20 in the dining survey
    - the number of successes $y$: 12 in the dining survey
    - $p$: the list of 6 plausible values $p = \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}$

## R for the likelihood cont'd

- The values are placed in the ```Likelihood``` column of the ```bayes_table``` data frame

```{r, echo = TRUE, warning = FALSE, message = FALSE}
bayes_table$Likelihood <- dbinom(12, size = 20, 
                                 prob = bayes_table$p)
bayes_table
```


## Posterior distribution for proportion $p$

- Bayes' rule for a discrete parameter:
$$\pi(p_i \mid y)  = \frac{\pi(p_i) \times L(p_i)} {\sum_j \pi(p_j) \times L(p_j)}$$
    - $\pi(p_i)$ is the prior probability of $p = p_i$
    - $L(p_i)$ is the likelihood function evaluated at $p = p_i$
    - $\pi(p_i \mid y)$ is the posterior probability of $p = p_i$ given the number of successes $y$
    - by the **Law of Total Probability**, the denominator gives the marginal distribution of the observation $y$. 

## Bayes' rule

- Bayes' rule can also be expressed as "prior times likelihood":
$$\pi(p_i \mid y)  \propto \pi(p_i) \times L(p_i)$$

## Posterior probability calculation
- First, calculate the denominator and denote the value as $D$.
$$D = \pi(0.3) \times L(0.3) + \pi(0.4) \times L(0.4) + \cdots + \pi(0.8) \times L(0.8)$$

- Then the posterior probability of $p = 0.3$ is given by
$$\pi(p = 0.3 \mid 12) = \frac{\pi(0.3) \times L(0.3)}{D} \approx 0.005$$

- In a similar fashion, the posterior probability of $p=0.5$ is calculated as
$$\pi(p = 0.5 \mid 12) = \frac{\pi(0.5) \times L(0.5)}{D} \approx 0.310$$

## R for posterior probability calculation

- Use the ```bayesian_crank()``` function to compute the posterior probabilities

```{r, echo = TRUE, warning = FALSE, message = FALSE}
bayesian_crank(bayes_table) -> bayes_table
bayes_table
```

## Comparing prior and psoterior

```{r, echo = FALSE, warning = FALSE, message = FALSE}
prior_post_plot(bayes_table, Color = crcblue) +
  increasefont()
```

## Inference: students' dining preference

- What is the posterior probability that over half of the students prefer eating out on Friday?  

- i.e. one is interested in the probability that $p >$ 0.5, in the posterior

- Looking at the table, this posterior probability is equal to
$$Prob(p > 0.5) \approx 0.463 + 0.147 + 0.029 = 0.639$$

- This means the owner is reasonably confident (with probability 0.639) that over half of the college students prefer to eat out on Friday

## Inference: students' dining preference cont'd

- Obtain the probability from the R output

```{r, echo = TRUE, warning = FALSE, message = FALSE}
sum(bayes_table$Posterior[bayes_table$p > 0.5])
```

## Discussion: issues with discrete priors

- If a plausible value is not specified in the prior distribution, it will be assigned a probability of zero in the posterior

- It generally is more desirable to have $p$ to be any value in [0, 1] including less plausible values such as $p = 1.0$

- To make this happen, the proportion $p$ should be allowed to take any value between 0 and 1, which means $p$ will be a continuous variable

- i.e. it is necessary to construct a continuous prior distribution for $p$

- A popular class of continuous prior distributions for proportion is: the beta distribution
