---
title: Chapter 11.7 Bayesian Inferences with Simple Linear Regression
author: Jim Albert and Monika Hu
date: Chapter 11 Simple Linear Regression
output: 
    beamer_presentation: default
    logo: ProbBayes_cover.jpg
fontsize: 12pt
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(ggplot2)
library(ProbBayes)
library(dplyr)
library(runjags)
library(coda)
library(tidyverse)
library(ggridges)
crcblue <- "#2905a1"
```



```{r message = FALSE, warning = FALSE, echo = FALSE, results = 'hide'}
PriceAreaData <- read_csv("house_prices.csv")
PriceAreaData$newsize <- PriceAreaData$size / 1000

modelString <-"
model {
## sampling
for (i in 1:N){
   y[i] ~ dnorm(beta0 + beta1*x[i], invsigma2)
}
## priors
beta0 ~ dnorm(mu0, g0)
beta1 ~ dnorm(mu1, g1)
invsigma2 ~ dgamma(a, b)
sigma <- sqrt(pow(invsigma2, -1))
}"

y <- PriceAreaData$price  
x <- PriceAreaData$newsize   
N <- length(y)  
the_data <- list("y" = y, "x" = x, "N" = N,
                 "mu0" = 0, "g0" = 0.0001,
                 "mu1" = 0, "g1" = 0.0001,
                 "a" = 1, "b" = 1)

posterior <- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("beta0", 
                                  "beta1", "sigma"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000)

post <- as.mcmc(posterior)
```

## Simulate fits from the regression model

- The intercept $\beta_0$ and slope $\beta_1$ determine the linear relationship between the mean of the response $Y$ and the predictor $x$
\begin{equation}
E(Y) = \beta_0 + \beta_1 x
\end{equation}

- Each pair of values ($\beta_0, \beta_1$) corresponds to a line $\beta_0 + \beta_1 x$ in the space of values of $x$ and $y$

- Posterior means: $\tilde {\beta_0}$ and $\tilde {\beta_1}$

- The line 
\begin{equation*}
y = \tilde{\beta_0} + \tilde{\beta_1} x
\end{equation*}
corresponds to a ``best" line of fit through the data

## Simulate fits from the regression model cont'd

- This best line represents a most likely value of the line $\beta_0 + \beta_1 x$ from the posterior\index{posterior} distribution

- How about the uncertainty of this line estimate?

- We can draw a sample of $J$ rows from the matrix of  posterior draws of $(\beta_0, \beta_1)$ and collecting the line estimates
\begin{equation*}
\tilde{\beta_0}^{(j)} + \tilde{\beta_1}^{(j)} x, 
\end{equation*}
where $j = 1, ..., J$

## Simulate fits from the regression model cont'd

```{r size = "footnotesize", eval = FALSE}
post <- as.mcmc(posterior)
post_means <- apply(post, 2, mean)
post <- as.data.frame(post)
ggplot(PriceAreaData, aes(newsize, price)) +
  geom_point(size=3) +
  geom_abline(data=post[1:10, ],
              aes(intercept=beta0, slope=beta1),
              alpha = 0.5) +
  geom_abline(intercept = post_means[1],
              slope = post_means[2],
              size = 2) +
  ylab("Price") + xlab("Size") +
  theme_grey(base_size = 18, base_family = "")
```

## Simulate fits from the regression model cont'd

```{r size = "footnotesize", echo = FALSE, fig.height = 3, fig.width = 4.5}
post <- as.mcmc(posterior)
post_means <- apply(post, 2, mean)
post <- as.data.frame(post)
ggplot(PriceAreaData, aes(newsize, price)) +
  geom_point(size=1) +
  geom_abline(data=post[1:10, ],
              aes(intercept=beta0, slope=beta1),
              alpha = 0.5) +
  geom_abline(intercept = post_means[1],
              slope = post_means[2],
              size = 1) +
  ylab("Price") + xlab("Size") +
  theme_grey(base_size = 10, base_family = "")
```

- Variation among the ten fits

- What happens with a larger sample size?

## Learning about the expected response

- Learn about the expected response $E(Y)$ for a specific value of the predictor $x$

- How?

- We can obtain a simulated sample from the posterior of $\beta_0 + \beta_1 x$ by computing this linear function, $E(Y) = \beta_0 + \beta_1 x$, on each of the simulated pairs from the posterior of $(\beta_0, \beta_1)$


## Learning about the expected response cont'd

- Suppose we are interested in the expected price $E(Y)$ for a house with a  size of 1, i.e. $x = 1$ (1000 sq feet)

```{r, size = "footnotesize"}
size <- 1
mean_response <- post[, "beta0"] + 
  size * post[, "beta1"]
```

## Learning about the expected response cont'd

```{r size = "footnotesize", eval = FALSE}
one_expected <- function(x){
  lp <- post[ , "beta0"] +  x * post[ , "beta1"]
  data.frame(Value = paste("Size =", x),
             Expected_Price = lp)
}
df <- map_df(c(1.2, 1.6, 2.0, 2.4), one_expected)

ggplot(df, aes(x = Expected_Price, y = Value)) +
  geom_density_ridges(fill = crcblue,
                      color = "white") +
  theme_grey(base_size = 18, base_family = "")
```

- Density plots of the simulated posterior samples for the expected  prices  $E(Y \mid 1.2)$, $E(Y \mid 1.6)$, $E(Y \mid 2.0)$, $E(Y \mid 2.4)$ for these four house sizes.  

## Learning about the expected response cont'd

```{r size = "footnotesize", echo = FALSE, fig.height = 3, fig.width = 4.5, warning = FALSE}
one_expected <- function(x){
  lp <- post[ , "beta0"] +  x * post[ , "beta1"]
  data.frame(Value = paste("Size =", x),
             Expected_Price = lp)
}
df <- map_df(c(1.2, 1.6, 2.0, 2.4), one_expected)

ggplot(df, aes(x = Expected_Price, y = Value)) +
  geom_density_ridges(fill = crcblue,
                      color = "white") +
  theme_grey(base_size = 10, base_family = "")
```

## Learning about the expected response cont'd

```{r size = "footnotesize"}
df %>% group_by(Value) %>%
  summarize(P05 = quantile(Expected_Price, 0.05),
            P50 = median(Expected_Price),
            P95 = quantile(Expected_Price, 0.95))
```


## Prediction of future response

- So far, we have seen
    - the variability among the fitted lines 
    - the variability among the simulated house price for fixed size (reflects the variability in the posterior draws of $\beta_0$ and $\beta_1$)

- To predict future values for a house sale price $Y$ given its size $x$, we also need to incorporate the sampling model in the simulation process
\begin{equation}
Y_i \mid \beta_0, \beta_1, \sigma \overset{ind}{\sim} \textrm{Normal}(\beta_0 + \beta_1 x_i, \sigma)
\end{equation}

## Prediction of future response cont'd

\begin{eqnarray*}
\text{simulate}\,\, E[y]^{(1)} = \beta_0^{(1)} + \beta_1^{(1)} x &\rightarrow& \text{sample}\,\, \tilde{y}^{(1)} \sim {\rm{Normal}}(E[y]^{(1)}, \sigma^{(1)})\\
\text{simulate}\,\, E[y]^{(2)} = \beta_0^{(2)} + \beta_1^{(2)} x &\rightarrow& \text{sample}\,\, \tilde{y}^{(2)} \sim {\rm{Normal}}(E[y]^{(2)}, \sigma^{(2)})\\
&\vdots& \\
\text{simulate}\,\, E[y]^{(S)} = \beta_0^{(S)} + \beta_1^{(S)} x &\rightarrow& \text{sample}\,\, \tilde{y}^{(S)} \sim {\rm{Normal}}(E[y]^{(S)}, \sigma^{(S)})\\
\end{eqnarray*}

## Prediction of future response cont'd

```{r size = "footnotesize"}
one_predicted <- function(x){
  lp <- post[ , "beta0"] +  x * post[ , "beta1"]
  y <- rnorm(5000, lp, post[, "sigma"])
  data.frame(Value = paste("Price =", x),
             Predicted_Price = y)
}
```

## Prediction of future response cont'd

```{r size = "footnotesize", echo = FALSE, fig.height = 3, fig.width = 4.5, warning = FALSE}
one_predicted <- function(x){
  lp <- post[ , "beta0"] +  x * post[ , "beta1"]
  y <- rnorm(5000, lp, post[, "sigma"])
  data.frame(Value = paste("Size =", x),
             Predicted_Price = y)
}
set.seed(123)
df <- map_df(c(1.2, 1.6, 2.0, 2.4), one_predicted)

ggplot(df, aes(x = Predicted_Price, y = Value)) +
  geom_density_ridges(fill = crcblue,
                      color = "white") +
  theme_grey(base_size = 10, base_family = "")
```

## Prediction of future response cont'd

```{r size = "footnotesize"}
df %>% group_by(Value) %>%
  summarize(P05 = quantile(Predicted_Price, 0.05),
            P50 = median(Predicted_Price),
            P95 = quantile(Predicted_Price, 0.95))
```

- The prediction interval is substantially wider than the posterior interval - why?

## Posterior predictive model checking

- Review: 
    - helpful in judging the suitability of the linear regression model 
    - the observed response values should be consistent with predicted responses generated from the fitted model

\pause

- Two steps to get a replicated sample (same sample size):

1. Values of the parameters $(\beta_0, \beta_1, \sigma)$ are simulated from the posterior distribution -- call these simulated values $(\beta^*_0, \beta^*_1, \sigma^*)$

2. A sample $\{y_1^R, ..., y_n^R\}$ is simulated where the sample size is $n = 24$ and $y_i^R$ is Normal($\mu^*_i, \sigma^*)$, where $\mu^*_i = \beta^*_0 + \beta^*_1 x_i$.

## Posterior predictive model checking cont'd

```{r size = "footnotesize", echo = FALSE, fig.height = 3, fig.width = 4.5, warning = FALSE}
rep_data <- function(j){
  k <- sample(5000, size = 1)
  lp <- post[k , "beta0"] +
    PriceAreaData$newsize * post[ k, "beta1"]
  y <- rnorm(24, lp, post[k , "sigma"])
  data.frame(Type = paste("Simulation", j),
             size = PriceAreaData$newsize,
             price = y)
}

df <- map_df(1:8, rep_data)
df_obs <- select(PriceAreaData, newsize, price) %>%
  mutate(Type = "Observed",
         size = newsize) %>%
  select(Type, size, price)
rbind(df, df_obs) -> DF

ggplot(DF, aes(size, price)) +
  geom_point(size=1, color = crcblue) +
  facet_wrap(~ Type, ncol = 3) +
  theme_grey(base_size = 10, base_family = "")
```

- Your conclusion?

## Predictive residuals

- Consider the observed point ($x_i, y_i$)

- Is the observed response value $y_i$ consistent with predictions $\tilde{y}_i$ of this observation from the fitted model?

\pause

- We can simulate predictions  $\tilde{y}_i$ from the posterior predictive distribution in two steps:  

1. One simulates $(\beta_0, \beta_1, \sigma)$ from the posterior distribution 
2. One simulates $\tilde{y}_i$ from a normal distribution with mean $\beta_0 + \beta_1 x_i$ and standard deviation $\sigma$ 

- By repeating this process many times, we have a sample of values \{$\tilde{y}_i$\} from the posterior predictive distribution

## Predictive residuals cont'd

- Compute the predictive residual
\begin{equation}
r_i = y_i - \tilde{y}_i
\end{equation}

- If this predictive residual is away from zero, that indicates that the observation is not consistent with the linear regression model

## Predictive residuals cont'd

```{r size = "footnotesize", echo = FALSE, fig.height = 3, fig.width = 4.5, warning = FALSE}
one_p_residual <- function(x, y){
  lp <- post[ , "beta0"] +  x * post[ , "beta1"]
  yp <- rnorm(5000, lp, post[, "sigma"])
  residual <- y - yp
  s_residual <- quantile(residual, c(.05, .95))
  data.frame(Value = x,
             LO = s_residual[1],
             HI = s_residual[2])
}
df <- map2_df(x, y, one_p_residual)

ggplot(df, aes(x, LO)) +
  geom_linerange(aes(ymin = LO,
                     ymax = HI),
                 color = crcblue,
                 size = 1.3) +
  xlab("Size") +
  ylab("Predictive Residual") +
  geom_hline(yintercept = 0, color = "black", size = 1.5) +
  increasefont(Size = 10)
```

- Your conclusion?

