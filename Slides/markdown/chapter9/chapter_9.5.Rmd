---
title: "Chapter 9.5 Gibbs Sampling"
author: "Jim Albert and Monika Hu"
date: "Chapter 9 Simulation by Markov Chain Monte Carlo"
output:
  beamer_presentation: default
fontsize: 12pt
---

## Introduction

- Here we introduce an MCMC algorithm\index{MCMC!algorithm} for simulating from a probability distribution of several variables.

- The Gibbs sampling algorithm is based on conditional distributions.

- It facilitates parameter estimation in Bayesian models with more than one parameter.

## Bivariate discrete distribution

-Suppose that the random variables $X$ and $Y$ each take on the values 1, 2, 3, 4

- The joint probability distribution is given in the following table.

|       |       |       |       |       |
| ----: | ----: | ----: | ----: | ----: |
|       |       |       |       |       |
| \(Y\) |     1 |     2 |     3 |     4 |
|     1 | 0.100 | 0.075 | 0.050 | 0.025 |
|     2 | 0.075 | 0.100 | 0.075 | 0.050 |
|     3 | 0.050 | 0.075 | 0.100 | 0.075 |
|     4 | 0.025 | 0.050 | 0.075 | 0.100 |


- Suppose it is of interest to simulate from this joint distribution of $(X, Y)$.  

## Gibbs Sampling - Step 1

- Set up a Markov chain by taking simulated draws from the conditional distributions $f(x \mid y)$ and $f(y \mid x)$.    

- Suppose the algorithm starts at the value $X = 1$.

- [Step 1]  One simulates $Y$ from the conditional distribution $f(y \mid X = 1)$ that is proportional to:

| \(Y\) | Probability |
| ----: | ----------: |
|     1 |       0.100 |
|     2 |       0.075 |
|     3 |       0.050 |
|     4 |       0.025 |

- Suppose we perform this simulation and obtain $Y = 2$.

## Gibbs Sampling - Step 2

- [Step 2]  Next one simulates $X$ from the conditional distribution of $f(x \mid Y = 2).$ This distribution is found by looking at the probabilities in the second row of the probability matrix.

| \(X\)       |     1 |     2 |     3 |     4 |
| :---------- | ----: | ----: | ----: | ----: |
| Probability | 0.075 | 0.100 | 0.075 | 0.050 |

- Suppose the simulated draw from this distribution is $X = 3$.

## Repeat

- By implementing Steps 1 and 2, we have one iteration of Gibbs sampling, obtaining the simulated pair $(X, Y) = (3, 2)$.  

- We repeat Steps 1 and 2 many times where we condition in each case on the most recently simulated values of $X$ or $Y$.

- This defines a Markov chain that moves from one simulated pair $(X^{(j)}, Y^{(j)})$ to the next simulated pair $(X^{(j+1)}, Y^{(j+1)})$.  In theory,  this process will converge to the joint probability distribution of $(X, Y)$.

## R Function

- Write a short R function ```gibbs_discrete()```\index{gibbs_discrete()} to implement Gibbs sampling for a two-parameter discrete distribution\index{two-parameter!distribution} where the probabilities are represented in a matrix. 

- One inputs the matrix ```p``` and the output is a matrix of simulated draws of $X$ and $Y$ where each row corresponds to a simulated pair.  

## R Function gibbs_discrete()

```
gibbs_discrete <- function(p, i = 1, iter = 1000){
  x <- matrix(0, iter, 2)
  nX <- dim(p)[1]
  nY <- dim(p)[2]
  for(k in 1:iter){
    j <- sample(1:nY, 1, prob = p[i, ])
    i <- sample(1:nX, 1, prob = p[, j])
    x[k, ] <- c(i, j)
  }
  x
}
```

## Running Function

- The function ```gibbs_discrete()``` is run using the probability matrix for our example.  

- The relative frequencies\index{relative frequency} of these pairs are good approximations to the joint probabilities\index{joint!probability}.

```
sp <- data.frame(gibbs_discrete(p))
names(sp) <- c("X", "Y")
table(sp) / 1000
    Y
X       1     2     3     4
  1 0.086 0.058 0.050 0.020
  2 0.061 0.081 0.079 0.048
  3 0.046 0.070 0.090 0.079
  4 0.017 0.036 0.068 0.111
```

## Beta-Binomial Sampling

- The Gibbs sampling algorithm works for any two-parameter distribution. 

- Suppose we flip a coin $n$ times and observe $y$ heads where the probability of heads is $p$, and our prior\index{prior} for the heads probability is described by a Beta($a, b$) curve.

$$
Y \mid p \sim  \textrm{Binomial}(n, p),
 p \sim \textrm{Beta}(a, b).
$$

## Conditional distributions

- Need to identify the two conditional distributions $Y \mid p$ and $p \mid Y$.  

- First write down the joint density of $(Y, p)$.

$$
f(Y = y, p) = \pi(p)f(Y = y \mid p) \nonumber 
$$

$$
=  \left[\frac{1}{B(a, b)} p^{a - 1} (1-p)^{b-1}\right] \left[{n \choose y} p^y (1 - p)^{n-y}\right]. \nonumber
$$

- The conditional density $f(Y = y \mid p)$ is $\textrm{Binomial}(n, p)$.

- The conditional density $\pi(p \mid y)$ is proportional to
$$
p^{y + a - 1} (1 - p)^{n - y + b - 1},
$$
which we recognize as a Beta distribution with  parameters $y + a$ and $n - y + b$.  

## The Gibbs sampling

- It is straightforward to write an algorithm to implement Gibbs sampling.  Suppose that the current simulated value of $p$ is $p^{(j)}$.

- Simulate $Y^{(j)}$ from a $\textrm{Binomial}(20, p^{(j)})$ distribution.

```
y <- rbinom(1, size = 20, prob = p)
```

- Given the current simulated value $y^{(j)}$, simulate $p^{(j+1)}$ from a Beta distribution with shape parameters $y^{(j)} + 5$ and $20 - y^{(j)} + 5$.

```
p <- rbeta(1, y + a, n - y + b)
```

## R function

- R function ```gibbs_betabin()``` will implement Gibbs sampling for this problem.  

- One inputs the sample size $n$ and the shape parameters $a$ and $b$.  

```
gibbs_betabin <- function(n, a, b, p = 0.5, iter = 1000){
  x <- matrix(0, iter, 2)
  for(k in 1:iter){
    y <- rbinom(1, size = n, prob = p)
    p <- rbeta(1, y + a, n - y + b )
    x[k, ] <- c(y, p)
  }
  x
}
```

## Example

- Run Gibbs sampling for this Beta-Binomial model with $n = 20$, $a = 5$, and $b = 5$. 

- The matrix  ```sp``` is an approximate simulated sample from the joint distribution of $Y$ and $p$.  The histogram of simulated draws in Figure represents a sample from the marginal distribution $f(y)$.

```
sp <- data.frame(gibbs_betabin(20, 5, 5))
```

```{r,  echo = FALSE, out.width = 150}
knitr::include_graphics("figures/mcmc5.png")
```

## Normal sampling  -- both parameters unknown**

- Consider the situation of sampling from a Normal distribution with mean $\mu$ and standard deviation $\sigma$ are unknown.  

- Suppose we take a sample of $n$ observations $Y_1, .., Y_n$ from a Normal distribution with mean $\mu$ and variance\index{variance} $\sigma^2$.  

$$
f(y_i \mid \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{1}{2 \sigma^2}(y_i - \mu)^2\right\}.
$$

- It will be convenient to reexpress the variance $\sigma$ by the __precision__  $\phi$ where
$$
\phi = \frac{1}{\sigma^2}.
$$

- The precision $\phi$ reflects the strength in knowledge about the location of the observation $Y_i$.

## Prior density

- Next step is to construct a prior density on the parameter vector $(\mu, \phi)$.  

- Assume that one's opinion about the location of the mean $\mu$ is independent\index{independent} of one's belief about the location of the precision $\phi$.  

- Assume that $\mu$ and $\phi$ are independent, so one  writes the joint prior density\index{joint!prior} as
$$
\pi(\mu, \phi) = \pi_{\mu}(\mu) \pi_{\phi}(\phi),
$$
- Assume that $\mu$ is Normal with mean $\mu_0$ and precision $\phi_0$ and the precision parameter $\phi$ is assumed Gamma\index{Gamma} with parameters $a$ and $b$.

## Likelihood

- The likelihood\index{likelihood} is the density of these Normal observations viewed as a function of the mean $\mu$ and the precision parameter $\phi$.  

$$
        L(\mu, \phi) =\prod_{i=1}^n \frac{\sqrt{\phi}}{\sqrt{2 \pi}} \exp\left\{-\frac{\phi}{2}(y_i - \mu)^2\right\} 
$$

$$
         \propto \phi^{n/2} \exp\left\{-\frac{\phi}{2}\sum_{i=1}^n (y_i - \mu)^2\right\}.
$$

## Gibbs sampling

- Write down the expression for the posterior density as the product of the likelihood and prior where any constants are removed.

$$
\pi(\mu, \phi \mid y_1, \cdots, y_n ) \propto  \phi^{n/2} \exp\left\{-\frac{\phi}{2}\sum_{i=1}^n (y_i - \mu)^2\right\} 
$$

$$
  \times  \exp\left\{-\frac{\phi_0}{2}(\mu - \mu_0)^2\right\}  \phi^{a-1} \exp(-b \phi).
$$


- Identify the two conditional posterior distributions $\pi(\mu \mid \phi, y_1, \cdots, y_n)$ and $\pi(\phi \mid \mu, y_1, \cdots, y_n)$.

## Conditional distributions

- The first conditional density follows from the work in Chapter 8 on Bayesian inference about a mean with a conjugate prior when the sampling standard deviation is known. 

- The conditional distribution $\pi(\mu \mid \phi, y_1, \cdots, y_n)$ is Normal with mean
$$
\mu_n = \frac{\phi_0 \mu_0  + n \phi \bar y }{\phi_0  + n \phi}.
$$
and standard deviation
$$
\sigma_n = \sqrt{\frac{1}{\phi_0  + n \phi}}.
$$

## Conditional distributions

- Collecting terms, the second conditional density $\pi(\phi \mid \mu, y_1, \cdots, y_n)$ is proportional to
$$
\pi(\phi \mid \mu, y_1, \cdots y_n) \propto \phi^{n/2 + a - 1} \exp\left\{-\phi\left[\frac{1}{2}\sum_{i=1}^n (y_i- \mu)^2 + b\right]\right\}. 
$$

- This is a Gamma density\index{Gamma!density} with parameters
$$
a_n = \frac{n}{2} + a, \, \,
b_n = \frac{1}{2}\sum_{i=1}^n (y_i - \mu)^2 + b.
$$


## R function

- R function ```gibbs_normal()```\index{gibbs_normal()} is written to implement this Gibbs sampling.  

- The inputs are a list ```s``` containing the vector  ```y``` and the prior parameters ```mu0```, ```phi0```, ```a```, and ```b```the starting value ```phi``` and the number of Gibbs sampling iterations ```S```.  

- This function is similar to the ```gibbs_betabin()``` function -- the two simulations in the Gibbs sampling are accomplished by use of the ```rnorm()```\index{rnorm()} and ```rgamma()}\index{rgamma()} functions.

## gibbs_normal() function

```
gibbs_normal <- function(s, phi = 0.002, iter = 1000){
  ybar <- mean(s$y);  n <- length(s$y)
  mu0 <- s$mu0;  phi0 <- s$phi0
  a <- s$a;  b <- s$b
  x <- matrix(0, iter, 2)
  for(k in 1:iter){
   mun <- (phi0 * mu0 + n * phi * ybar) /
      (phi0 + n * phi)
    sigman <- sqrt(1 / (phi0 + n * phi))
    mu <- rnorm(1, mean = mun, sd = sigman)
    an <- n / 2 + a
    bn <- sum((s$y - mu) ^ 2) / 2 + b
    phi <- rgamma(1, shape = an, rate = bn)
    x[k, ] <- c(mu, phi)
  }
  x
}
```

## Example

- Run this function for our Buffalo snowfall\index{Buffalo snowfall} example

- Prior assumes that $\mu$ and the precision $\phi$ are independent, where $\mu$ is Normal with mean 10 and standard deviation 3 (i.e. precision $1/3^2$), and $\phi$ is Gamma with $a =  b = 1$.  

- Output of this function is a matrix ```out``` where the two columns of the matrix correspond $\mu$ and $\phi$.

```
s <- list(y = data$JAN, mu0 = 10, phi0 = 1/3^2, 
          a = 1, b = 1)
out <- gibbs_normal(s, iter=10000)
```

## Posterior of parameters

- By transforming $\sigma = \sqrt{1 / \phi}$, one  obtains a sample of the simulated draws of the standard deviation $\sigma$.  

- Figure  displays a scatterplot\index{scatterplot} of the posterior draws of $\mu$ and $\sigma$.


```{r,  echo = FALSE, out.width = 200}
knitr::include_graphics("figures/mcmc9.png")
```

